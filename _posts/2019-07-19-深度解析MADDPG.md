### 深度解析MADDPG

### MADDPG原理

OpenAI 2017论文[《Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments》](https://arxiv.org/abs/1706.02275)

[知乎](https://zhuanlan.zhihu.com/p/53811876)已经比较详细介绍了







本文主要分析该开源程序的架构，使其更好的实用

程序[github](https://github.com/openai/maddpg) 

环境[github](https://github.com/openai/multiagent-particle-envs)



### 程序架构

两个主要部分 env、trainer

### ENV 部分

![Scenarios（ Multiagent文件夹）](C:\Users\Administrator\Desktop\Scenarios（ Multiagent文件夹）.png)









### make_env，

调用MultiAgent下的environment中的MultiAgentEnv 建立环境 该env是gym env的子环境
该环境下存在 reset 和 step函数 





而该函数的输入是  

```python
world=Scenario.make_world()
reset_callback=scenario.reset_world, 
reward_callback=scenario.reward,
observation_callback= scenario.observation,
info_callback=None,
done_callback=None, 
shared_viewer=True
```
这几个都是scenario 中的函数 

Scenario 是环境的类  该类下 包含两个函数 make_world 和 reset_world

每个子环境 都是通过调用Scenario 然后再调用



 首先利用mark world 建立整个环境




