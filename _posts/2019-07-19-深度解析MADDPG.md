# 深度解析MADDPG

## MADDPG原理

OpenAI 2017论文[《Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments》](https://arxiv.org/abs/1706.02275)

[知乎](https://zhuanlan.zhihu.com/p/53811876)已经比较详细介绍了

本文主要分析该开源程序的架构，为了使其更好的实用，写本博客。

该程序主要是对 文章中的environment的实验进行房展

在试验中 存在 agent 和landmarks 

实验一：speak 不能动 但是可以看，listener说一个颜色 listener 就到一个颜色的landmark上去

 ![1563521975121](H:\git_code\原来版本\_posts\1563521975121.png)



实验二：在这种经典的捕食者-食饵游戏变体中，N个较慢的Predator在Landmark阻碍下追赶速度更快的Prey

每一个Predator碰撞到prey，代理人得到奖励，而对手受到惩罚。

![1563522168803](H:\git_code\原来版本\_posts\1563522168803.png)

实验三 agent必须通过进行合作，以达到Landmark。必须躲避其他agent 到达相同的地标。

![1563522277123](H:\git_code\原来版本\_posts\1563522277123.png)

实验四：N个agent要到N个landmark上去，然而，一个adversary也希望到达目标地标，这个adversary不知道哪个地标是正确的。agent 根据adversary到正确地点的距离受到负reward。

![1563522473848](H:\git_code\原来版本\_posts\1563522473848.png)

算法[github](https://github.com/openai/maddpg) 

环境[github](https://github.com/openai/multiagent-particle-envs)



## 程序架构

强化学习的主要两个部分即 环境 和 算法

环境 主要是通过 主函数中 make-env创建

```python
def make_env(scenario_name, arglist, benchmark=False):
    from multiagent.environment import MultiAgentEnv
    import multiagent.scenarios as scenarios

    # load scenario from script
    scenario = scenarios.load(scenario_name + ".py").Scenario()
    # create world
    world = scenario.make_world()
    # create multiagent environment
    if benchmark:
        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, scenario.benchmark_data)
    else:
        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)
    return env
```



算法 主要是通过主函数中 get_trainers创建

```python
def get_trainers(env, num_adversaries, obs_shape_n, arglist):
    trainers = []
    model = mlp_model
    trainer = MADDPGAgentTrainer
    for i in range(num_adversaries):
        trainers.append(trainer(
            "agent_%d" % i, model, obs_shape_n, env.action_space, i, arglist,
            local_q_func=(arglist.adv_policy=='ddpg')))
    for i in range(num_adversaries, env.n):
        trainers.append(trainer(
            "agent_%d" % i, model, obs_shape_n, env.action_space, i, arglist,
            local_q_func=(arglist.good_policy=='ddpg')))
    return trainers
```







## ENV 部分

![Scenarios（ Multiagent文件夹）](H:\git_code\zachary2wave.github.io\_posts\Scenarios（ Multiagent文件夹）.png)

<center>总体概况图<center>



在ENV的部分调用过程中，都是通过调用具体环境，即Scenario下的子类环境，然后用该场景下的两个方法

Make-world 和 reset-world。创建具体的环境。

总体流程是 

1. 调用环境 make-env（train）
2. **具体环境建立**
3. Make-world 创建 agent 初始化各个agent
4. reset-world初始化各个agent
5. **创建env** 
6. 为每个agent创建 状态-动作空间
7.  **step 和reward等环境**

### 具体子环境

#### Make_World

创建过程中

1、循环调用core中的agent 向 world 中增加 agent 和 landmark 

agent 包含以下参数

+ movable： True or Flase 可以不可以移动 
+ silent ：True or Flase       可以不可以交流
+ Bline  ：True or Flase       可以不可以观察
+ Unoise、Cnoise ：True or Flase  U动作噪声 C 交流噪声
+ state 状态
+ action 动作

2、world 中定义了 agent运动的基本方式，主要利用两个函数

+ apply_action_force： 将动作加上噪声
+ apply_environment_force ：将agent的 运动 加在状态上，需要通过get_collision_force判断是不是碰撞
+ integrate_state  ： 求积分 更改 将速度增加在 P上
+ update_agent_state：将交流动作加上噪声

#### reset world

利用循环将各个agent  的动作 通信动作 初始化

####   定义奖励 和 观察



### Environment

将world 输入到 MultiagentEnv或者BatchMultiAgentEnv 以创建

该函数的输入是  

```python
world=Scenario.make_world()
reset_callback=scenario.reset_world, 
reward_callback=scenario.reward,
observation_callback= scenario.observation,
info_callback=scenario.done
done_callback=None, 
shared_viewer=True
```
该环境下存在 reset 和 step函数 

该step也是调用 world 下的 step , 但是该处的step可以确定agent的动作顺序

```python
for i, agent in enumerate(self.agents):
    self._set_action(action_n[i], agent, self.action_space[i])
```

## Trainer

在这里每一个agent都需要建立一个trainer ，在trainer list中

```python
trainer(
    name = "agent_%d" % i, 
    model = model, 
    obs_shape_n =  [env.observation_space[i].shape for i in range(env.n)]   # env.n 是agent的个数 这个地方传进去一个list
    env.action_space =  env.action_space
    i = agent的序号 
    arglist=None
    local_q_func = (arglist.adv_policy=='ddpg'))
```

子函数：

+ P—train

+ Q—trian

+ discount_with_dones

+  make_update_exp

### P_train Critic

```python
self.act, self.p_train, self.p_update, self.p_debug = p_train(
    scope=self.name,  所传入agent的名字
    make_obs_ph_n = obs_ph_n,   这个地方传入的是所有agent的observation的tensor 
    act_space_n = act_space_n,  所有agent的动作空间
    p_index=agent_index,
    p_func = model,   
    q_func = model,
    optimizer=tf.train.AdamOptimizer(learning_rate=args.lr),
    grad_norm_clipping=0.5,
    local_q_func=local_q_func,   是否使用 本地 Qfun
    num_units=args.num_units
)
```



这个地方调用 p_fun() 是之前直接建立的网络model    （train的 函数）

 其中输入input 是一个 tensor

```python
def mlp_model(input, num_outputs, scope, reuse=False, num_units=64, rnn_cell=None):

    with tf.variable_scope(scope, reuse=reuse):
        out = input
        out = layers.fully_connected(out, num_outputs=num_units, activation_fn=tf.nn.relu)
        out = layers.fully_connected(out, num_outputs=num_units, activation_fn=tf.nn.relu)
        out = layers.fully_connected(out, num_outputs=num_outputs, activation_fn=None)
        return out
```

中间过程非常复杂 













最后形成了3个网络 

+ act网络  actor 

+ train   critic
+ update_target_p











### Q_train Actor























